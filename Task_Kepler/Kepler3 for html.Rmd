---
title: "Analysis Plan for Kepler Data"
author: "Aliaksandr Hubin"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r}
#devtools::install_github("https://github.com/jonlachmann/GMJMCMC/tree/stratos")
library(FBMS) 
library(tidyverse) 
library(DataExplorer)
#setwd("Task_Kepler")
```

# Load the data and perform EDA

```{r}
data = read.csv("https://raw.githubusercontent.com/OpenExoplanetCatalogue/oec_tables/master/comma_separated/open_exoplanet_catalogue.txt")


head(data)
DataExplorer::plot_str(data = data)
DataExplorer::plot_intro(data = data) 
DataExplorer::plot_missing(data = data)
DataExplorer::plot_density(data[,-2],nrow = 2,ncol = 3)
DataExplorer::plot_density(log(data[,c("eccentricity","mass","period","semimajoraxis","hoststar_radius","hoststar_mass")]),nrow = 2,ncol = 3)
DataExplorer::plot_correlation(data[,-2],type = "continuous",cor_args = list(use = "pairwise.complete.obs", method = "pearson"))

```

# Select relevant columns for analysis

We shall keep 300 observations as a hold out test set and the rest as the training set

```{r}
data <- data %>% select(semimajoraxis,mass, radius, period, eccentricity, hoststar_mass, hoststar_radius, hoststar_metallicity,hoststar_temperature,binaryflag) %>% na.omit()
summary(data)

set.seed(1)
te.ind <- sample.int(n = 939,size = 300,replace = F)
data.train = data[-te.ind,]
data.test = data[te.ind,]
```

# Iteration 1: Simple Bayesian Gaussian regression model with model averaging and Jeffreys prior

```{r}
set.seed(1)
blr <- FBMS::fbms(semimajoraxis ~ ., data = data.train,beta_prior = list(type = "Jeffreys-BIC"), N = 5000)
plot(blr)
summary(blr,labels = names(data.train)[-1],effects = c(0.025,0.975),tol = 0)

```

### Check stability

```{r}
set.seed(1)
all.probs <- sapply(1:20,FUN = function(x)FBMS::fbms(semimajoraxis ~ ., data = data.train,beta_prior = list(type = "Jeffreys-BIC"), N = 5000)$marg.probs)

chain_means <- rowMeans(all.probs)
chain_sd <- sapply(1:9, function(i) sd(all.probs[i,]))

alpha_upper <- chain_means + 1.96*chain_sd
alpha_lower <- chain_means - 1.96*chain_sd

stability <- data.frame(covariate = names(data)[-1],one.chain = round(blr$marg.probs[1,],4),mean = round(chain_means,4),lower = round(alpha_lower,4),upper = round(alpha_upper,4))

print(stability)
```

Bayesian Linear regression BMA results appear fairly stable, allowing us to continue with their discussion.

### Marginal Inclusion Probabilities

The marginal inclusion probabilities indicate the likelihood that each predictor is included in the model. High probabilities suggest that the predictor is likely important for explaining the response variable.

1.  **period** is included in the model with absolute certainty. The orbital period is a fundamental characteristic of an orbit and strongly influences the semimajor axis due to Kepler's third law, which relates the square of the period to the cube of the semimajor axis.

2.  **mass** is also almost certainly included in the model. The mass of the orbiting body can affect the dynamics of the system, influencing the semimajor axis through gravitational interactions.

3.  **eccentricity** has a high inclusion probability. The eccentricity of an orbit describes its deviation from a perfect circle, which can affect the semimajor axis as it alters the orbital shape.

4.  **hoststar_metallicity** has a posterior inclusion above 0.5 and would be stably a part of the MPM. Hard to interpret its input on the semi major axes of the planet.

    ...

5.  **hoststar_mass** has a posterior inclusion below 0.0001 that would indicate it is not important, which is a bit counter-intuitive as it should hold the planetary system together.

### Quantiles of Effect Sizes

The quantiles of model averaged effect sizes of posterior modes across all models provide the 2.5% and 97.5% quantiles for the posterior distribution of each coefficient. We shall look at the predictors with marginal inclusion probabilities above 0.5

1.  **mass**: The positive interval indicates that as the mass increases, the semimajor axis is expected to increase. This makes sense physically, as a larger mass could imply a more substantial gravitational influence, potentially affecting the orbit's size.

2.  **period**: The very narrow interval indicates a precise positive effect of the orbital period on the semimajor axis, consistent with Kepler's third law.

3.  **eccentricity**: The wide interval, spanning from zero to a significant positive value, indicates uncertainty about the effect of eccentricity, but it can have a large positive effect.

    But let us additionally look at the 95% CrI for the median probability model, which under Jeffreys priors approximately correspond to the 95% CI, allowing us to easily obtain the estimates using the lm function in R

    ```{r}
    confint(lm(semimajoraxis ~ 1 + mass + period + eccentricity+hoststar_metallicity, data = data.train))
    ```

essentially supporting the conclusions of model averaged posterior modes.

### Make predictions

```{r}
preds.train <- predict(blr, as.matrix(data.train[,-1]), link = function(x) x)
preds.test <- predict(blr, as.matrix(data.test[,-1]))
r.blr <- round(c(cor(data.train[,1],preds.train$mean)^2,cor(data.test[,1],preds.test$mean)^2),3)
plot(x = data.train[, 1], preds.train$mean, xlab = "data", ylab = "prediction", main = "Title of the Plot", col = "blue", pch = 16)
points(x = data.test[, 1], preds.test$mean, col = "red", pch = 17)
legend("topright", legend = c(paste0("Training Data, R.sqr = ",r.blr[1]), paste0("Test Data, R.sqr = ",r.blr[2])), col = c("blue", "red"), pch = c(16, 17))
```

The predictions show extremely good predictive ability of the model for both training and testing data. Yet, let us reflect on potential pitfalls of the model do decide if we are interested in it.

### Possible criticism

The finding that the host star's mass is not important in predicting the semimajor axis does seem counterintuitive, as the mass of the host star should, in theory, have a significant influence on the orbit of the planets around it. Here are some potential reasons and considerations to critically evaluate this finding and whether it motivates the use of a more complex model:

1.  **Data Quality and Completeness**:

    -   **Missing or Inaccurate Data?** Yet we assumed missing at random for all missing data. But if there are inaccuracies or biases in missing data for host star mass, might it explain why this predictor is not showing importance? We believe it is not important even if we were missing the systems with high or low solar mass systematically as the solar mass would still be much higher than the mass of a planet. The only way it could be important is if the solar mass was constant or having close to zero variance. Yet as we saw in EDA, this is not the case.

2.  **Confounding Variables**:

    -   **Collinearity**: If host star mass is highly correlated with other predictors (e.g., period or eccentricity), its unique contribution might be overshadowed, leading to an underestimation of its importance. Again, this is not the case according to EDA.

3.  **Model Simplicity**:

    -   **Linear Model Limitations**: A simple linear model might not capture the complex relationships between host star mass and the semimajor axis, particularly if the relationship is non-linear or interacts with other variables. We could try more complex models. Multiplicative effects for the original model could be testes by a model with log transformed responses. Additive non-linearities can be tested through through fractional polynomials. Finally, more complicated non-linear relationships with both non-linearities and interactions can be tested by a symbolic regression on BGNLM. Let us dig into this.

# Iteration 2: Bayesian Gaussian regression model with model averaging and Jeffreys and prior log-transformed response

Inference

```{r}
set.seed(1)
log.data.train <- log(abs(data.train)+.Machine$double.xmin)
log.data.test <- log(abs(data.test)+.Machine$double.xmin)
blr.log <- FBMS::fbms( semimajoraxis ~ .,beta_prior = list(type = "Jeffreys-BIC"), data = log.data.train, N = 5000)
plot(blr.log)
summary(blr.log,labels = names(data.train)[-1],effects = c(0.025,0.975))
```

## Check stability

```{r}
set.seed(1)
all.probs <- sapply(1:20,FUN = function(x)FBMS::fbms(semimajoraxis ~ .,beta_prior = list(type = "Jeffreys-BIC"), data = log.data.train, N = 5000)$marg.probs)

chain_means <- rowMeans(all.probs)
chain_sd <- sapply(1:9, function(i) sd(all.probs[i,]))

alpha_upper <- chain_means + 1.96*chain_sd
alpha_lower <- chain_means - 1.96*chain_sd

stability <- data.frame(covariate = names(data)[-1],one.chain = round(blr.log$marg.probs[1,],4),mean = round(chain_means,4),lower = round(alpha_lower,4),upper = round(alpha_upper,4))

print(stability)
```

The results appear stable

#### 1. **Inclusion Probabilities:**

-   **High Inclusion Probabilities:** log of Period has perfect inclusion probabilities, indicating they are crucial predictors for the log-transformed semimajor axis.

-   **Strong Predictors:** log of hoststar_mass is also above 0.5 and thus corresponds to the median probability model

#### 2. **Effect Sizes:**

-   **Period:** around 0.66 corresponding to the power of 2/3 on an orignial scale

-   **Hoststar Mass:** around 0.33 corresponding to the power of 1/3 on an original scale

    let us also fit the median probability model here

    ```{r}
    mpm <- lm(semimajoraxis ~ 1 + period + hoststar_mass, data = log.data.train)
    summary(mpm)
    confint(mpm)
    ```

corroborating the model averaged conclusions.

-   **Better Fit to Physical Laws:** The transformed model better aligns with astrophysical principles, making the results more interpretable and reliable. On the original scale we get very close to the functional form of the third Kepller's law stating that $$a \propto (P^2*M)^{1/3}$$with $$a$$ being the semi major axis, $$P$$ being the period of rotation and $$M$$ being the solar mass.

### Predictions

```{r}
preds.train.log <- predict(blr.log, as.matrix(log.data.train[,-1]),link = exp)
preds.test.log <- predict(blr.log, as.matrix(log.data.test[,-1]),link = exp)
r.blr.log <- round(c(cor(data.train[,1],preds.train.log$mean)^2,cor(data.test[,1],preds.test.log$mean)^2),3)
print(r.blr.log)
plot(x = data.train[, 1], preds.train.log$mean,ylim = c(min(preds.train$mean),max(preds.train$mean)), xlab = "data", ylab = "prediction", main = "Title of the Plot", col = "blue", pch = 16)
points(x = data.test[, 1], preds.test.log$mean, col = "red", pch = 17)
legend("topright", legend = c(paste0("Training Data, R.sqr = ",r.blr.log[1]), paste0("Test Data, R.sqr = ",r.blr.log[2])), col = c("blue", "red"), pch = c(16, 17))
```

The model as expected gave perfect predictions on the original scale as we essentially recovered the law.

Here, we were somewhat lucky since the law was easy to discover on the log-transformed scale.

Let us still finish the steps that we were planning to do prior to transforming the covariates and the responses. We proceed on the original scale.

# Iteration 3: More complex functional forms with Bayesian methods

Bayesian fractional polynomials

```{r}
transforms <- c("p0","p2","p3","p05","pm05","pm1","pm2","p0p0","p0p05","p0p1","p0p2","p0p3","p0p05","p0pm05","p0pm1","p0pm2")
probs <- gen.probs.gmjmcmc(transforms)
probs$gen <- c(0,1,0,1) # Only modifications!
params <- gen.params.gmjmcmc(ncol(data.train)-1)
params$feat$D <- 1   # Set depth of features to 1
set.seed(1)
bfp <- FBMS::fbms(semimajoraxis ~ ., data = data.train,beta_prior = list(type = "Jeffreys-BIC"),method = "gmjmcmc.parallel",transforms = transforms,runs = 20,cores = 10,P = 20, probs = probs, params = params)
plot(bfp)
summary(bfp,labels = names(data.train)[-1])
```

Check convergence

```{r}
diagn_plot(bfp,window = 10,FUN = median)
diagn_plot(bfp,window = 10,FUN = max)
```

We see convergence after 17th generation of GMJMCMCwe see the same important effects as in a shorter run. Let us check convergence

Convergence seems rather stable for this class of models on this data set and with these tuning parameters of the sampler.

#### 1. **Importance of Period**:

-   The predictor `period` has an extremely high inclusion probability, indicating it is almost certainly a key predictor for the semimajor axis.

-   Fractional polynomial transformations of `period`, specifically `p0p05(period) and`p3(period) also show notable inclusion probability. This suggests that non-linear transformations of `period`are important for capturing the relationship with the semimajor axis.

#### 2. **Other Predictors**:

Predictors such as `mass`, `radius`, `hoststar_mass`, and some transformations of them seem important and while all of them make sense in terms of the true law, the true law was not feasible in the space of fractional polynomials.

`hoststar_metallicity`, `hoststar_temperature`, `hoststar_radius`, and `eccentricity` have very low inclusion probabilities, all less than 0.001.

But let us look at MPM here as the model, just like for the model averaged effect, we see positive effect of period and its polynomial term, which makes sense.

```{r}
confint(lm(semimajoraxis ~ 1 + period + p0p05(period) + p3(period) + hoststar_mass + p0p3(hoststar_mass) + radius + p0pm1(mass), data = data.train))
```

### Predictions

```{r}
preds.train.bfp <- predict(bfp, data.train[,-1])
preds.test.bfp <- predict(bfp, data.test[,-1])
r.bfp <- round(c(cor(data.train[,1],preds.train.bfp$aggr$mean)^2,cor(data.test[,1],preds.test.bfp$aggr$mean)^2),3)
plot(x = data.train[, 1], preds.train.bfp$aggr$mean, xlab = "data", ylab = "prediction", main = "Title of the Plot", col = "blue", pch = 16)
points(x = data.test[, 1], preds.test.bfp$aggr$mean, col = "red", pch = 17)
legend("topright", legend = c(paste0("Training Data, R.sqr = ",r.bfp[1]), paste0("Test Data, R.sqr = ",r.bfp[2])), col = c("blue", "red"), pch = c(16, 17))
```

The predictions are excellent and even slightly better than for the linear model, yet worse than for the log transformed data used in the linear model.

The limitation of a BFP model is the lack of interactions. So let us try out Bayesian generalized nonlinear models that allow to both model non-linearity and interactions.

### Bayesian generalized nonlinear models

```{r}
transforms <- c("sin_deg","exp_dbl","p0","troot","p3")
probs <- gen.probs.gmjmcmc(transforms)
params <- gen.params.gmjmcmc(ncol(data.train)-1)
params$loglik$var = "unknown"
set.seed(1)
bgnlm <- FBMS::fbms(semimajoraxis ~ ., data = data.train,method = "gmjmcmc.parallel",beta_prior = list(type = "Jeffreys-BIC"),transforms = transforms,runs = 20,cores = 8,P = 20, probs = probs, params = params)
plot(bgnlm)
summary(bgnlm,labels = names(data.train)[-1])
```

check convergence

```{r}
diagn_plot(bgnlm,window = 10,FUN = median)
diagn_plot(bgnlm,window = 10,FUN = max)
```

we see possible convergence for the later generations of GMJMCMC, but let us increase the compute to check if it is indeed so. The limitation of the convergence statistics is that it may show perfect convergence upon algorithm stuck in a good mode and not mixing futher across the modes.

```{r}
set.seed(1)
bgnlm <- FBMS::fbms(semimajoraxis ~ ., data = data.train,method = "gmjmcmc.parallel",beta_prior = list(type = "Jeffreys-BIC"),transforms = transforms,runs = 64,cores = 8,P = 40,probs = probs, params = params)
plot(bgnlm)
summary(bgnlm,labels = names(data.train)[-1])
```

```{r}
diagn_plot(bgnlm,window = 10,FUN = median)
diagn_plot(bgnlm,window = 10,FUN = max)
```

Median probability model

```{r}
confint(lm(semimajoraxis ~ 1 +I(troot(period*period*hoststar_mass)),data = data.train))
```

```{r}
preds.train.bgnlm <- predict(bgnlm, data.train[,-1])
preds.test.bgnlm <- predict(bgnlm, data.test[,-1])
r.bgnlm <- round(c(cor(data.train[,1],preds.train.bgnlm$aggr$mean)^2,cor(data.test[,1],preds.test.bgnlm$aggr$mean)^2),3)
plot(x = data.train[, 1], preds.train.bgnlm$aggr$mean, xlab = "data", ylab = "prediction", main = "Title of the Plot", col = "blue", pch = 16)
points(x = data.test[, 1], preds.test.bgnlm$aggr$mean, col = "red", pch = 17)
legend("topright", legend = c(paste0("Training Data, R.sqr = ",r.bgnlm[1]), paste0("Test Data, R.sqr = ",r.bgnlm[2])), col = c("blue", "red"), pch = c(16, 17))
```

\
In the Bayesian Generalized Nonlinear Model (BGNLM) analysis, you obtained the following results:

**Predictor: `troot(((period*hoststar_mass)*period))`** **Inclusion Probability: 1.000000**

This result indicates a perfect inclusion probability (1.0) for the predictor `troot(((period*hoststar_mass)*period))`, suggesting that this complex non-linear interaction term is crucial for predicting the semimajor axis.

### Interpretation

#### 1. **Complex Interaction Term**:

-   The predictor `troot(((period*hoststar_mass)*period))` combines `period` and `hoststar_mass` in a multiplicative form, followed by a transformation.

-   `troot` likely represents a specific non-linear transformation, such as a root function, which means the predictor is a transformed version of the product of `period`, `hoststar_mass`, and `period` again.

### Discussion with Relation to Kepler's Third Law

#### 1. **Kepler's Third Law**

Kepler's Third Law states that the square of the orbital period of a planet is proportional to the cube of the semimajor axis of its orbit.

#### 2. **Implications and Interpretation**

-   **Alignment with Physical Laws**: The inclusion of `troot(((period*hoststar_mass)*period))` in the BGNLM model supports Kepler’s Third Law by explicitly incorporating the cubic root transformation of the complicated interaction. This suggests that the model correctly captures the underlying (known in this case as Kepler's Third Law) physical relationship between the orbital period and the semimajor axis.

-   **Predictive Power and Physical Meaning**: The perfect inclusion probability of this term indicates that the model effectively leverages the cubic root relationship to predict the semimajor axis. This reinforces the physical validity of the model and emphasizes the importance of incorporating both the period and the host star’s mass in a non-linear fashion.

#### 2. **Comparison with Previous Models**:

-   **Linear and Log-Transformed Models**: These simpler models highlighted the importance of `period` and `hoststar_mass` individually, but the BGNLM model shows that their interaction, particularly in a non-linear form, is even more crucial.

-   **Fractional Polynomials**: The fractional polynomial model also indicated non-linear relationships but did not capture this specific interaction and it also missed the solar mass in its functional form, highlighting the added value of BGNLM in uncovering complex dependencies.

### Predictions

```{r}
preds.train.bgnlm <- predict(bgnlm, data.train[,-1])
preds.test.bgnlm <- predict(bgnlm, data.test[,-1])
r.bgnlm <- round(c(cor(data.train[,1],preds.train.bgnlm$aggr$mean)^2,cor(data.test[,1],preds.test.bgnlm$aggr$mean)^2),3)
plot(x = data.train[, 1], preds.train.bgnlm$aggr$mean, xlab = "data", ylab = "prediction", main = "Title of the Plot", col = "blue", pch = 16)
points(x = data.test[, 1], preds.test.bgnlm$aggr$mean, col = "red", pch = 17)
legend("topright", legend = c(paste0("Training Data, R.sqr = ",r.bgnlm[1]), paste0("Test Data, R.sqr = ",r.bgnlm[2])), col = c("blue", "red"), pch = c(16, 17))
```

The predictions are expectidely perfect on both training and testing sets, as inclusion of `troot(((period*hoststar_mass)*period))` in the BGNLM model effectively captures the essence of Kepler’s Third Law, incorporating the orbital period and the host star’s mass in a cubic root transformation. This predictor reflects the key physical relationship between these variables, demonstrating that the model is not only statistically sound but also physically meaningful. The high inclusion probability underscores the importance of this non-linear interaction in accurately predicting the semimajor axis.

And use the g prior and redo the inference.

```{r}
set.seed(1)
bgnlm <- FBMS::fbms(semimajoraxis ~ ., data = data.train,method = "gmjmcmc.parallel",beta_prior = list(type = "g-prior", g = nrow(data.train)),transforms = transforms,runs = 64,cores = 8,P = 40,probs = probs, params = params)
plot(bgnlm)
summary(bgnlm,labels = names(data.train)[-1])

```

Here we perfectly recover the true law! Also good convergence for g - prior.

```{r}
diagn_plot(bgnlm,window = 10,FUN = median)
diagn_plot(bgnlm,window = 10,FUN = max)
```

And as expected perfect predictions

```{r}
preds.train.bgnlm <- predict(bgnlm, data.train[,-1])
preds.test.bgnlm <- predict(bgnlm, data.test[,-1])
r.bgnlm <- round(c(cor(data.train[,1],preds.train.bgnlm$aggr$mean)^2,cor(data.test[,1],preds.test.bgnlm$aggr$mean)^2),3)
plot(x = data.train[, 1], preds.train.bgnlm$aggr$mean, xlab = "data", ylab = "prediction", main = "Title of the Plot", col = "blue", pch = 16)
points(x = data.test[, 1], preds.test.bgnlm$aggr$mean, col = "red", pch = 17)
legend("topright", legend = c(paste0("Training Data, R.sqr = ",r.bgnlm[1]), paste0("Test Data, R.sqr = ",r.bgnlm[2])), col = c("blue", "red"), pch = c(16, 17))
```

We see that variation is possible in the results and that ideally maximal possible compute resources should be used to reduce the variation. But given enough resources GMJMCMC converges to being able to recover the true underlying physical law.
